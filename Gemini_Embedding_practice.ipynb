{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "yq-KkqnXk9_3"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "djYCCJaWujJr"
      },
      "source": [
        "#### 연습 문장"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "cId8JiZi1I9f"
      },
      "outputs": [],
      "source": [
        "input_str = \"I love holiday\"\n",
        "ouput_str = \"나는 연휴를 사랑해\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "ZSGnIk5M1J0j"
      },
      "outputs": [],
      "source": [
        "# dot1 = np.dot(\"I\", \"나는\") # the underlying ufunc (multiply) only works with numbers — not text\n",
        "# dot2 = np.dot(\"love\", \"나는\")\n",
        "# dot3 = np.dot(\"holiday\", \"나는\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PXz5HG491_em"
      },
      "source": [
        "## Most of all, I have to tokenize each word to represent the meaning as a number."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2pwRl27b2KOk"
      },
      "source": [
        "[Gemini Embedding Reference URL](https://ai.google.dev/gemini-api/docs/embeddings?hl=ko)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`export GEMINI_API_KEY=<YOUR_API_KEY_HERE>`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HbuB3nBH1vh9",
        "outputId": "3fd65f32-9f56-4ea5-b2a4-3316733616a4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ContentEmbedding(\n",
            "  values=[\n",
            "    -0.022374554,\n",
            "    -0.004560777,\n",
            "    0.013309286,\n",
            "    -0.0545072,\n",
            "    -0.02090443,\n",
            "    <... 3067 more items ...>,\n",
            "  ]\n",
            ")]\n"
          ]
        }
      ],
      "source": [
        "from google import genai\n",
        "\n",
        "client = genai.Client(api_key=GEMINI_API_KEY)\n",
        "\n",
        "result = client.models.embed_content(\n",
        "        model=\"gemini-embedding-001\",\n",
        "        contents=\"What is the meaning of life?\")\n",
        "\n",
        "print(result.embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EKUbzcvz3Zm3",
        "outputId": "d19174a5-4851-4570-94c2-c8775473397b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The Gemini Embedding Model, like other powerful embedding models, translates textual information into numerical representations called \"embeddings\" or \"vectors.\" These vectors capture the semantic meaning of the text such that texts with similar meanings are located closer together in a high-dimensional space, while dissimilar texts are further apart.\n",
            "\n",
            "Let's break down how it works effectively and then compare it to other sources.\n",
            "\n",
            "---\n",
            "\n",
            "### 1. How the Gemini Embedding Model Works Concretely\n",
            "\n",
            "At its core, the Gemini Embedding Model leverages a sophisticated deep learning architecture, primarily based on the **Transformer encoder** design, which has proven highly effective for natural language understanding.\n",
            "\n",
            "Here's a step-by-step breakdown:\n",
            "\n",
            "1.  **Tokenization:**\n",
            "    *   **Input:** You provide a piece of text (e.g., \"The quick brown fox jumps over the lazy dog.\")\n",
            "    *   **Process:** The model first breaks down this text into smaller units called \"tokens.\" These can be words, sub-word units (like \"un-\", \"happy\"), or even individual characters, depending on the tokenizer. Each token is then mapped to a numerical ID.\n",
            "    *   **Example:** \"The quick brown fox\" -> [token_id_for_The, token_id_for_quick, ...]\n",
            "\n",
            "2.  **Positional Encoding:**\n",
            "    *   **Problem:** Transformers process tokens in parallel, so they inherently lose information about the order of words.\n",
            "    *   **Solution:** Positional encodings are added to the token embeddings. These are special vectors that inject information about each token's position in the sequence, allowing the model to understand word order and context.\n",
            "\n",
            "3.  **Transformer Encoder Layers:**\n",
            "    *   **Core:** This is where the magic happens. The token embeddings (with positional encodings) are fed into multiple layers of a Transformer encoder.\n",
            "    *   **Self-Attention:** Each layer consists primarily of a \"multi-head self-attention\" mechanism. This mechanism allows the model to weigh the importance of all other tokens in the input sequence when processing a specific token.\n",
            "        *   **Example:** When processing \"bank\" in \"river bank,\" the model will pay more attention to \"river\" and less to other irrelevant words, helping it disambiguate the meaning of \"bank\" (not a financial institution).\n",
            "    *   **Feed-Forward Networks:** After self-attention, the output of each token passes through a feed-forward neural network, which further processes the contextual information.\n",
            "    *   **Layer Stacking:** These layers are stacked, allowing the model to build increasingly abstract and sophisticated representations of the text. Lower layers might capture grammatical structures, while higher layers grasp semantic relationships and overall meaning.\n",
            "\n",
            "4.  **Pooling/Output Layer:**\n",
            "    *   **Problem:** The Transformer encoder outputs a sequence of vectors, one for each token. We need a single, fixed-size vector for the entire input text.\n",
            "    *   **Solution:** A \"pooling\" strategy is applied. The most common and effective method for semantic embeddings is **mean pooling**, where the vectors for all tokens in the sequence are averaged to produce a single, dense vector. Other methods might include using a special `[CLS]` token's final representation (as in original BERT) or more complex learned pooling mechanisms.\n",
            "    *   **Output:** This final fixed-size vector (e.g., 768 dimensions, 1024 dimensions) is the embedding for your input text.\n",
            "\n",
            "5.  **Training Objective (The \"Why\" it's Effective):**\n",
            "    *   **Contrastive Learning:** This is crucial for semantic embedding models. Gemini (like many other state-of-the-art models) is pre-trained on massive, diverse datasets using contrastive learning objectives.\n",
            "        *   **Process:** The model is given pairs or triplets of text. It's trained to pull semantically similar texts closer together in the embedding space and push dissimilar texts further apart.\n",
            "        *   **Example:** If you give it a query and a relevant document (positive pair), and the same query with an irrelevant document (negative pair), the model learns to generate embeddings where the query and positive document are numerically \"close,\" and the query and negative document are \"far apart.\"\n",
            "    *   **Massive Data:** Google's access to vast and diverse datasets (web text, books, code, conversational data, potentially multimodal data due to the Gemini family's nature) allows it to learn highly generalized and nuanced semantic representations.\n",
            "\n",
            "---\n",
            "\n",
            "### 2. How Gemini Achieves Effectiveness (Key Strengths)\n",
            "\n",
            "1.  **Unparalleled Scale and Data Diversity:** Google's resources enable pre-training on truly colossal datasets, encompassing a vast range of topics, styles, and languages. This breadth allows the model to develop a highly robust and generalizable understanding of semantics. The sheer volume helps it capture rare patterns and common sense knowledge.\n",
            "2.  **Advanced Training Techniques:** Beyond basic contrastive learning, Google likely employs proprietary and cutting-edge self-supervised and semi-supervised learning techniques that further refine the embedding space, making it more semantically accurate and robust to noise. The integration with the broader Gemini research likely means innovations from the main multimodal model inform the text embedding variant.\n",
            "3.  **Semantic Fidelity and Nuance:** Due to extensive training on diverse data, Gemini models excel at capturing subtle semantic differences, handling polysemy (words with multiple meanings, e.g., \"bank\" as in river vs. financial), and understanding complex queries.\n",
            "4.  **Robustness and Generalization:** It performs well across various domains and text types (formal, informal, technical, creative) without needing specific fine-tuning for many common tasks. This means it can effectively embed a news article, a social media post, or a code snippet.\n",
            "5.  **Multilinguality (for specific versions):** Many top-tier embedding models, including Gemini variants, are trained on multilingual datasets, making them effective for cross-lingual tasks where you might search for a document in one language using a query in another.\n",
            "\n",
            "---\n",
            "\n",
            "### 3. Comparing to Other Sources\n",
            "\n",
            "When comparing Gemini Embedding Models (e.g., `text-embedding-gemini-001` or its latest iterations) to other prominent sources, we look at several criteria:\n",
            "\n",
            "#### A. OpenAI (e.g., `text-embedding-ada-002`, `text-embedding-3-small`, `text-embedding-3-large`)\n",
            "\n",
            "*   **Similarity:** Both Gemini and OpenAI models use Transformer architectures and are trained with contrastive learning on massive datasets. They are both highly competitive and often achieve state-of-the-art results on standard benchmarks like MTEB (Massive Text Embedding Benchmark).\n",
            "*   **Key Differentiators:**\n",
            "    *   **Performance:** OpenAI's `text-embedding-3-large` has shown extremely strong performance, often topping MTEB leaderboards. Gemini models are also consistently at the top. The \"best\" often depends on the specific benchmark and the exact version of the model being tested.\n",
            "    *   **Dimensionality Options:** OpenAI offers options like 1536, 3072 (for `text-embedding-3-large`), or even reduced dimensions for efficiency. Gemini typically offers fixed dimensions (e.g., 768 or 1024). Higher dimensions *can* capture more nuance but increase storage and computational cost.\n",
            "    *   **Cost:** Both are API-based, pay-per-token. Pricing can be competitive, with OpenAI sometimes offering more aggressive pricing for their smaller, highly efficient models (like `text-embedding-3-small`).\n",
            "    *   **Ecosystem Integration:** OpenAI's models are deeply integrated into their ecosystem, popular with many developers. Gemini integrates naturally within Google Cloud and Google AI Studio.\n",
            "\n",
            "#### B. Hugging Face / Open-Source Models (e.g., Sentence-BERT variants, E5, BGE, Instructor-XL, Cohere models via HF)\n",
            "\n",
            "*   **Similarity:** Many open-source models (especially the larger ones like BGE-Large, E5-Large) are built on Transformer architectures and trained with similar contrastive learning objectives.\n",
            "*   **Key Differentiators:**\n",
            "    *   **Cost & Control:** Open-source models are free to download and run locally. This offers complete control over data privacy, fine-tuning, and avoids API costs. This is their *biggest advantage*.\n",
            "    *   **Performance:** Large, well-trained open-source models can be incredibly competitive, sometimes even outperforming commercial APIs on specific benchmarks. For example, some BGE or E5 variants are top performers on MTEB.\n",
            "    *   **Fine-tuning:** Open-source models are much easier to fine-tune on your specific domain data, potentially yielding superior performance for highly specialized tasks compared to a general-purpose commercial API.\n",
            "    *   **Resource Intensity:** Running large open-source models locally requires significant computational resources (GPUs, RAM). Commercial APIs abstract this away.\n",
            "    *   **Convenience:** Commercial APIs like Gemini are often easier to integrate, manage, and scale without needing to manage infrastructure.\n",
            "    *   **Updates:** Commercial APIs are continuously updated and improved by the provider. Open-source models rely on community contributions or specific research groups.\n",
            "\n",
            "#### C. Cohere Embed Models\n",
            "\n",
            "*   **Similarity:** Cohere offers powerful, API-based embedding models (`embed-english-v3.0`, `embed-multilingual-v3.0`) that are often on par with or even slightly superior to OpenAI and Gemini on certain benchmarks, particularly for long-context understanding.\n",
            "*   **Key Differentiators:**\n",
            "    *   **Context Window:** Cohere often boasts very large context windows, allowing it to embed much longer documents effectively.\n",
            "    *   **Performance:** Consistently high on benchmarks, particularly strong for semantic search.\n",
            "    *   **Pricing & Features:** Similar pay-per-token model, with specific features like reranking available in their ecosystem.\n",
            "\n",
            "---\n",
            "\n",
            "### Conclusion on Effectiveness\n",
            "\n",
            "The Gemini Embedding Model works effectively by:\n",
            "\n",
            "1.  **Leveraging a robust Transformer encoder architecture** for deep contextual understanding.\n",
            "2.  **Employing advanced contrastive learning objectives** to create a semantically meaningful embedding space.\n",
            "3.  **Being trained on Google's unparalleled scale of diverse and high-quality data,** allowing it to generalize across a vast range of topics and nuances.\n",
            "\n",
            "Compared to other sources, Gemini stands as a **top-tier commercial API** offering:\n",
            "*   **Convenience and scalability** like OpenAI.\n",
            "*   **State-of-the-art performance** that is highly competitive with the best models from OpenAI, Cohere, and the leading open-source alternatives.\n",
            "*   Its effectiveness lies in its ability to consistently provide high-quality, semantically rich embeddings that power advanced AI applications like RAG, semantic search, and recommendation systems, often with minimal effort from the developer.\n",
            "\n",
            "The \"best\" choice between Gemini, OpenAI, Cohere, or an open-source model often comes down to a trade-off between absolute performance on specific benchmarks, cost, development convenience, infrastructure control, and the need for domain-specific fine-tuning.\n"
          ]
        }
      ],
      "source": [
        "response = client.models.generate_content(\n",
        "    model=\"gemini-2.5-flash\", contents=\"Concretly explain how Gemini Embedding Model works effectively comparing to other sources\"\n",
        "    # contents=\"Explain how AI works in a few words\" >>> 'AI learns patterns from vast amounts of data to make predictions or perform tasks.'\n",
        ")\n",
        "print(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v3JtGjZA6DN9"
      },
      "source": [
        "## Extract Vectors with Gemini Embedding in earnest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s6EhN1MP8EoW",
        "outputId": "d865ab6b-9f73-4ae2-debd-0dff95f29dfd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'google.genai.types.EmbedContentResponse'>\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "EmbedContentResponse(\n",
              "  embeddings=[\n",
              "    ContentEmbedding(\n",
              "      values=[\n",
              "        -0.010612635,\n",
              "        0.0059433444,\n",
              "        0.005594396,\n",
              "        -0.07262013,\n",
              "        -0.0070909816,\n",
              "        <... 3067 more items ...>,\n",
              "      ]\n",
              "    ),\n",
              "  ],\n",
              "  sdk_http_response=HttpResponse(\n",
              "    headers=<dict len=10>\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 48,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "em_query = client.models.embed_content(model=\"gemini-embedding-001\", contents=\"나는\")\n",
        "print(type(em_query))\n",
        "em_query"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "roDmL6O3-9e8",
        "outputId": "61d7f0cb-a074-4dbf-ad23-18bfc2ba5308"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'list'>\n",
            "<class 'google.genai.types.ContentEmbedding'>\n"
          ]
        }
      ],
      "source": [
        "try1 = em_query.embeddings\n",
        "[try2] = em_query.embeddings\n",
        "\n",
        "print(type(try1))\n",
        "print(type(try2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qlurlzht8k5p",
        "outputId": "a2bb52e1-17fc-4fcb-c19a-1d3914fdc787"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "3072"
            ]
          },
          "execution_count": 49,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "[em_q] = em_query.embeddings\n",
        "em_Q = em_q.values\n",
        "len(em_Q)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z-jpo1rq9Lhj"
      },
      "source": [
        "> 문자열 목록으로 전달하여 한 번에 여러 청크의 임베딩을 생성할 수도 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jjjobcFb6CyU",
        "outputId": "1a6fe6c4-3cd0-46b2-8c86-b39bc773ed69"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "EmbedContentResponse(\n",
              "  embeddings=[\n",
              "    ContentEmbedding(\n",
              "      values=[\n",
              "        -0.005278527,\n",
              "        0.01615053,\n",
              "        0.011982469,\n",
              "        -0.074000314,\n",
              "        0.0026567031,\n",
              "        <... 3067 more items ...>,\n",
              "      ]\n",
              "    ),\n",
              "    ContentEmbedding(\n",
              "      values=[\n",
              "        -0.0053034346,\n",
              "        0.00989779,\n",
              "        -0.0020667156,\n",
              "        -0.05728348,\n",
              "        0.0014906529,\n",
              "        <... 3067 more items ...>,\n",
              "      ]\n",
              "    ),\n",
              "    ContentEmbedding(\n",
              "      values=[\n",
              "        -0.022153739,\n",
              "        -0.0012626448,\n",
              "        -0.012944534,\n",
              "        -0.067008846,\n",
              "        -0.017269898,\n",
              "        <... 3067 more items ...>,\n",
              "      ]\n",
              "    ),\n",
              "  ],\n",
              "  sdk_http_response=HttpResponse(\n",
              "    headers=<dict len=10>\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# em_I = client.models.embed_content(\n",
        "#         model=\"gemini-embedding-001\",\n",
        "#         contents=\"I\")\n",
        "# em_love = client.models.embed_content(\n",
        "#         model=\"gemini-embedding-001\",\n",
        "#         contents=\"love\")\n",
        "# em_holiday = client.models.embed_content(\n",
        "#         model=\"gemini-embedding-001\",\n",
        "#         contents=\"holiday\")\n",
        "\n",
        "em_keys = client.models.embed_content(\n",
        "        model=\"gemini-embedding-001\",\n",
        "        contents=[\"I\", \"love\", \"holiday\"])\n",
        "em_keys"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "1tQd_VxV-Tiy"
      },
      "outputs": [],
      "source": [
        "em_I, em_love, em_holiday = em_keys.embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KXnJ_tuE-pYq",
        "outputId": "9c169bc0-85e0-40ae-83c2-0979f53bf763"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "3072"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(em_I.values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "9MvUtcZ9-WmI"
      },
      "outputs": [],
      "source": [
        "dot1 = np.dot(em_I.values, em_Q)\n",
        "dot2 = np.dot(em_love.values, em_Q)\n",
        "dot3 = np.dot(em_holiday.values, em_Q)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pmvnrc8FBEeZ",
        "outputId": "a18294d1-474a-432e-eb3f-fa8ba1e4bbb2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "dot1 between \"I\" and \"나는\"\n",
            ": 0.6739418859925859\n",
            "\n",
            "dot2 \"love\" and \"나는\"\n",
            ": 0.6114853961970522\n",
            "\n",
            "dot3 \"holiday\" and \"나는\"\n",
            ": 0.5855955563786995\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(f\"dot1 between \\\"I\\\" and \\\"나는\\\"\\n: {dot1}\\n\")\n",
        "print(f\"dot2 \\\"love\\\" and \\\"나는\\\"\\n: {dot2}\\n\")\n",
        "print(f\"dot3 \\\"holiday\\\" and \\\"나는\\\"\\n: {dot3}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C1xTaQ631WvI"
      },
      "outputs": [],
      "source": [
        "# dot1 = np.dot(\"I\", \"나는\")      >>> np.dot does not work on strings!!\n",
        "# dot2 = np.dot(\"love\", \"나는\")\n",
        "# dot3 = np.dot(\"holiday\", \"나는\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qR2JZTAxCIAm"
      },
      "source": [
        "### Dim-deduction test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "LidIYv2ZCLYC"
      },
      "outputs": [],
      "source": [
        "from google import genai\n",
        "from google.genai import types\n",
        "\n",
        "em_result = client.models.embed_content(\n",
        "        model=\"gemini-embedding-001\",\n",
        "        contents=[\"나는\", \"I\", \"love\", \"holiday\"],\n",
        "        config=types.EmbedContentConfig(outputDimensionality=512))\n",
        "\n",
        "em1, em2, em3, em4 = em_result.embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IVdKV5vrCQdc",
        "outputId": "e5d64310-184c-4d42-aa9c-f7bd7ca97f97"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.12782337001717595\n",
            "0.11915654657890391\n",
            "0.11436667282861002\n"
          ]
        }
      ],
      "source": [
        "dot_1 = np.dot(em1.values, em2.values)\n",
        "dot_2 = np.dot(em1.values, em3.values)\n",
        "dot_3 = np.dot(em1.values, em4.values)\n",
        "\n",
        "print(dot_1)\n",
        "print(dot_2)\n",
        "print(dot_3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I5CC7NpbDic4"
      },
      "source": [
        "#### 작은 크기의 품질 보장\n",
        "\n",
        "> 정규화된 임베딩은 크기가 아닌 벡터 방향을 비교하여 더 정확한 의미 유사성을 생성합니다. <br>768, 1536을 비롯한 다른 차원의 경우 다음과 같이 임베딩을 정규화해야 합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d3OuUeRjDSZJ",
        "outputId": "997abc74-3a0e-42d4-dc25-7fe064d47858"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Normed embedding length: 512\n",
            "Norm of normed embedding: 1.000000\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from numpy.linalg import norm\n",
        "\n",
        "em1_values_np = np.array(em1.values)\n",
        "normed_em1 = em1_values_np / np.linalg.norm(em1_values_np)\n",
        "\n",
        "print(f\"Normed embedding length: {len(normed_em1)}\")\n",
        "print(f\"Norm of normed embedding: {np.linalg.norm(normed_em1):.6f}\") # Should be very close to 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7hK-QAQLELen",
        "outputId": "d349ef75-562e-4c69-f79d-41c56e949f75"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  <After normailzation of the reduced vectors>\n",
            "\n",
            "sim1: 0.6160727695573414\n",
            "sim2: 0.5639979865058795\n",
            "sim3: 0.5348007357962068\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from numpy.linalg import norm\n",
        "\n",
        "def normalize(v):\n",
        "  np_v = np.array(v) # list to numpy array\n",
        "  normed_v = np_v / np.linalg.norm(np_v)\n",
        "\n",
        "  return normed_v\n",
        "\n",
        "sim1 = np.dot(normed_em1, normalize(em2.values))\n",
        "sim2 = np.dot(normed_em1, normalize(em3.values))\n",
        "sim3 = np.dot(normed_em1, normalize(em4.values))\n",
        "\n",
        "print(\"  <After normailzation of the reduced vectors>\\n\")\n",
        "print(f\"sim1: {sim1}\")\n",
        "print(f\"sim2: {sim2}\")\n",
        "print(f\"sim3: {sim3}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LPuKiXsWG4Qq"
      },
      "source": [
        "> 이들은 성능이 임베딩 차원의 크기와 엄격하게 연결되어 있지 않으며, 낮은 차원이 높은 차원과 비슷한 점수를 달성할 수 있다고 함!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-syW2ctqF7SV"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
